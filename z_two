from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.utils.dates import days_ago

# Define the default_args for the DAG
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'retries': 1,
}

# Create the DAG instance
dag = DAG(
    'postgres_query_dag',
    default_args=default_args,
    schedule_interval=None,  # You can set the schedule_interval as needed
    catchup=False,  # Set to False if you don't want historical runs to run
)

# Define a PostgreSQL connection ID from your Airflow Connections
postgres_conn_id = 'your_postgres_connection_id'

# Define a Python function to execute the query and print the result
def execute_query_and_print_result(**context):
    # SQL query to select data from dag_run table
    sql_query = "SELECT * FROM dag_run LIMIT 5;"
    
    # Execute the SQL query
    execute_sql = PostgresOperator(
        task_id='execute_sql',
        postgres_conn_id=postgres_conn_id,
        sql=sql_query,
        dag=dag,
    )
    
    # Execute the query
    execute_sql.execute(context)
    
    # Fetch and print the result
    result = execute_sql.xcom_pull(context=context, key=None)
    print(result)

# PythonOperator to execute the query and print the result
execute_and_print = PythonOperator(
    task_id='execute_and_print',
    python_callable=execute_query_and_print_result,
    provide_context=True,
    dag=dag,
)
