from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.utils.dates import days_ago

# Define the default_args for the DAG
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'retries': 1,
}

# Create the DAG instance
dag = DAG(
    'postgres_query_dag',
    default_args=default_args,
    schedule_interval=None,  # You can set the schedule_interval as needed
    catchup=False,  # Set to False if you don't want historical runs to run
)

# Define a PostgreSQL connection ID from your Airflow Connections
postgres_conn_id = 'your_postgres_connection_id'

# SQL query to select data from the dag_run table
sql_query = "SELECT * FROM dag_run LIMIT 5;"

# PostgresOperator to execute the SQL query
execute_sql = PostgresOperator(
    task_id='execute_sql',
    postgres_conn_id=postgres_conn_id,
    sql=sql_query,
    dag=dag,
)

# Python function to print the result
def print_result(**context):
    ti = context['ti']
    result = ti.xcom_pull(task_ids='execute_sql')
    print(result)

# PythonOperator to print the result
print_query_result = PythonOperator(
    task_id='print_query_result',
    python_callable=print_result,
    provide_context=True,
    dag=dag,
)

# Set the task dependencies
execute_sql >> print_query_result
